{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Electronics=pd.read_csv('ratings_Electronics.csv')\n",
    "print(Electronics.head())\n",
    "Electronics.columns=['user','item','rating','timestamp']\n",
    "#print(Electronics.head())\n",
    "del Electronics['timestamp']\n",
    "#print(Electronics.head())\n",
    "\n",
    "Videogames=pd.read_csv('ratings_Video_Games.csv')\n",
    "Videogames.columns=['user','item','rating','timestamp']\n",
    "#print(Videogames.head())\n",
    "del Videogames['timestamp']\n",
    "#print(Videogames.head())\n",
    "data=Electronics.append(Videogames)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Electronics_Video_Games.csv', index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark import SparkContext, since\n",
    "from pyspark.rdd import RDD\n",
    "import pyspark.sql\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SQLContext\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SQLContext \n",
    "sc = pyspark.SparkContext('local')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = sc.textFile(\"Electronics_Video_Games.csv\").map(lambda line: line.split(\",\"))\n",
    "ratings = ratings.toDF(['userID','itemID','rating'])\n",
    "ratings.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings1=ratings.select('userID').distinct()\n",
    "ratings1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxIndex = ratings1.count()\n",
    "print(maxIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.types import *\n",
    "#ratings_filename=\"/Users/drakaris/Downloads/Electronics_Video_Games.csv\"\n",
    "#ratings_df_schema = StructType([StructField('userID', IntegerType()),StructField('movieID', IntegerType()),StructField('rating', DoubleType())])\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_index = ratings1.rdd.zipWithIndex()\n",
    "distinct_val = with_index.map(lambda x: (Row(userID = x[0][0], id =x[1])))\n",
    "distinct_val = distinct_val.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings_df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, inferSchema=False).schema(ratings_df_schema).load(ratings_filename)\n",
    "#ratings_df = raw_ratings_df.drop('Timestamp')\n",
    "#ratings_df.cache()\n",
    "distinct_val.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = ratings.join(distinct_val, ['userID'], \"left\")\n",
    "rdd1 = joined_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings2=ratings.select('itemID').distinct()\n",
    "ratings2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxIndex = ratings2.count()\n",
    "print(maxIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_index = ratings2.rdd.zipWithIndex()\n",
    "distinct_val1 = with_index.map(lambda x: (Row(itemID = x[0][0], id2 =x[1])))\n",
    "distinct_val1 = distinct_val1.toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_val1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df1 = joined_df.join(distinct_val1, ['itemID'], \"left\")\n",
    "rdd2 = joined_df1.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "\n",
    "\n",
    "#als=ALS(userCol=\"userID\",itemCol=\"movieID\", ratingCol=\"rating\", coldStartStrategy=\"drop\",nonnegative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df2=joined_df1.drop('itemID')\n",
    "#joined_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df3=joined_df2.drop('userID')\n",
    "joined_df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=joined_df3.select(\"id\",\"id2\",\"rating\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.save('/Users/drakaris/Downloads/Electronics_Video_Games.csv', 'com.databricks.spark.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation\\\n",
    "    import ALS,MatrixFactorizationModel, Rating\n",
    "    \n",
    "ratings = df.rdd.map(lambda x: Rating(int(x[0]),\\\n",
    "    int(x[1]), float(x[2])))    \n",
    "\n",
    "train, test = ratings.randomSplit([0.8,0.2])\n",
    "\n",
    "#print(train.count()) #70,005\n",
    "#print(test.count()) #29,995\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to cache the data to speed up training\n",
    "train.cache()\n",
    "#test.cache()\n",
    "\n",
    "#Setting up the parameters for ALS\n",
    "rank = 10 # Latent Factors to be made\n",
    "numIterations = 15# Times to repeat process\n",
    "regParam = 0.35\n",
    "#Create the model on the training data\n",
    "model = ALS.train(train, rank, numIterations, regParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.recommendUsers(242,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For User Y Find N Products to Promote\n",
    "model.recommendProducts(196,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict Single Product for Single User\n",
    "model.predict(196, 242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Multi Users and Multi Products\n",
    "# Pre-Processing\n",
    "pred_input = train.map(lambda x:(x[0],x[1]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lots of Predictions\n",
    "#Returns Ratings(user, item, prediction)\n",
    "pred = model.predictAll(pred_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_reorg = train.map(lambda x:((x[0],x[1]), x[2]))\n",
    "pred_reorg = pred.map(lambda x:((x[0],x[1]), x[2]))\n",
    "\n",
    "#Do the actual join\n",
    "true_pred = true_reorg.join(pred_reorg)\n",
    "\n",
    "#Need to be able to square root the Mean-Squared Error\n",
    "from math import sqrt\n",
    "\n",
    "MSE = true_pred.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "RMSE = sqrt(MSE)#Results in 0.7629908117414474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Set Evaluation\n",
    "#More dense, but nothing we haven't done before\n",
    "test_input = test.map(lambda x:(x[0],x[1])) \n",
    "pred_test = model.predictAll(test_input)\n",
    "test_reorg = test.map(lambda x:((x[0],x[1]), x[2]))\n",
    "pred_reorg = pred_test.map(lambda x:((x[0],x[1]), x[2]))\n",
    "test_pred = test_reorg.join(pred_reorg)\n",
    "test_MSE = test_pred.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "test_RMSE = sqrt(test_MSE)#1.0145549956596238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting 0.1% of all the users to reccomend them top 3 items and calculate coverage\n",
    "user_only=train.toDF()\n",
    "users, smthelse=user_only.randomSplit([0.001,0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userlist=users.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reccomend each user 3 items\n",
    "for row in userlist['user']: \n",
    "    a=model.recommendProducts(row,3)\n",
    "    top_k_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somelist=[]\n",
    "recommended_number = 0\n",
    "#getting total number of reccomended items\n",
    "for row in range(1, len(top_k_list)): \n",
    "    #print(row)\n",
    "    recommended_number += len(top_k_list[row])\n",
    "    for a in range(0, len(top_k_list[row])):\n",
    "        somelist.append(top_k_list[row][a].product)\n",
    "        #print(len(top_k_list[row]))\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myset = set(somelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(myset)/recommended_number"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
