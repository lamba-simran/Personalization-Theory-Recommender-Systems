{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.session.SparkSession'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "print(type(spark))\n",
    "csv = spark.read.csv(\"personalization_theory/ratings_Electronics.csv\")\n",
    "\n",
    "# csv2 = spark.read.csv(\"ratings_Video_Games.csv\")\n",
    "# csv = csv.union(csv2)\n",
    "csv = csv.selectExpr(\"_c0 as user\", \"_c1 as item\", \"_c2 as rating\", \"_c3 as time\")\n",
    "distinct_val = csv.select('item').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "changedTypedf = csv.withColumn(\"time_int\", csv[\"time\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1403395200.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changedTypedf.approxQuantile(\"time_int\", [0.5], 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = changedTypedf.filter(changedTypedf.time_int < 1403395200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = changedTypedf.filter(changedTypedf.time_int >= 1403395200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, LongType\n",
    "\n",
    "with_index = distinct_val.rdd.zipWithIndex()\n",
    "distinct_val = with_index.map(lambda x: (Row(item = x[0].item, id =x[1])))\n",
    "distinct_val = distinct_val.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525967\n"
     ]
    }
   ],
   "source": [
    "distinct_val.take(2)\n",
    "maxIndex = distinct_val.count()\n",
    "print(maxIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "csv = csv.orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_df = csv.join(distinct_val, ['item'], \"left\")\n",
    "rdd1 = joined_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(item='0439394422', user='A1TL721YECDIM8', rating='3.0', time='1389657600', id=2485),\n",
       " Row(item='0439394422', user='APDCEJMFDO2YT', rating='5.0', time='1285545600', id=2485),\n",
       " Row(item='7793224531', user='ACNP9LJ391F3L', rating='4.0', time='1287532800', id=0),\n",
       " Row(item='7793224531', user='A2QE3HV9GW264C', rating='5.0', time='1305936000', id=0),\n",
       " Row(item='7793224531', user='A4NLBWDS2USI', rating='5.0', time='1312502400', id=0),\n",
       " Row(item='7793224531', user='A2CZG9I6ERMOFA', rating='4.0', time='1284249600', id=0),\n",
       " Row(item='7793224531', user='A2CK2QW8OUZARL', rating='5.0', time='1397433600', id=0),\n",
       " Row(item='7793224531', user='AN3CN5M3U865J', rating='4.0', time='1329868800', id=0),\n",
       " Row(item='7793224531', user='AO8QRF4F2XXS1', rating='5.0', time='1265760000', id=0),\n",
       " Row(item='7793224531', user='AO4Q0RPYN3Y1N', rating='5.0', time='1321747200', id=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd1.map(lambda line: (line.user, (line.id, line.rating))).groupByKey().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A2HLOMXD15UQ5F', <pyspark.resultiterable.ResultIterable at 0x177e34c18>),\n",
       " ('A8QDWEWU0EZ4E', <pyspark.resultiterable.ResultIterable at 0x177e34c50>),\n",
       " ('AZT8EE93WXLJZ', <pyspark.resultiterable.ResultIterable at 0x177e34da0>),\n",
       " ('A3152KT8N27WTF', <pyspark.resultiterable.ResultIterable at 0x177e34828>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A8QDWEWU0EZ4E', <pyspark.resultiterable.ResultIterable object at 0x177e2ec88>]\n",
      "[(2515, '5.0')]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print(list(rdd2.take(2)[1]))\n",
    "pprint.pprint(list(rdd2.take(2)[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "def sv_format(x):\n",
    "    item = []\n",
    "    rating = []\n",
    "    for i in x:\n",
    "        item.append(i[0])\n",
    "        rating.append(int(float((i[1]))))\n",
    "    points = zip(item, rating)\n",
    "    sorted_points = sorted(points)\n",
    "    new_xs = [point[0] for point in sorted_points]\n",
    "    new_ys = [point[1] for point in sorted_points]\n",
    "\n",
    "    return new_xs, new_ys\n",
    "\n",
    "sparseVectorData = rdd2.map(lambda a :(a[0], sv_format(a[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "def sv_format(x):\n",
    "    item = {}\n",
    "    for i in x:\n",
    "        item[i[0]] = int(float((i[1])))\n",
    "    return item\n",
    "\n",
    "sparseVectorData = rdd2.map(lambda a :(a[0], sv_format(a[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[248] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparseVectorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile = sparseVectorData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myfile1 = myfile[:int(len(myfile)/10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('AKWMVB3BIPVJD', {111: 1, 13526: 5, 132171: 5, 196655: 5, 283926: 5})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(myfile1)\n",
    "my_df.to_csv('1-10.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this block of code generates random .5% of users with older ratings\n",
    "\"\"\"\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import rand \n",
    "import pandas as pd\n",
    "\n",
    "distinct_val = old.select('item').distinct()\n",
    "with_index = distinct_val.rdd.zipWithIndex()\n",
    "distinct_val = with_index.map(lambda x: (Row(item = x[0].item, id =x[1])))\n",
    "distinct_val = distinct_val.toDF()\n",
    "old = old.orderBy(rand())\n",
    "joined_df = old.join(distinct_val, ['item'], \"left\")\n",
    "rdd1 = joined_df.rdd\n",
    "rdd2 = rdd1.map(lambda line: (line.user, (line.id, line.rating))).groupByKey().cache()\n",
    "\n",
    "def sv_format(x):\n",
    "    item = {}\n",
    "    for i in x:\n",
    "        item[i[0]] = int(float((i[1])))\n",
    "    return item\n",
    "\n",
    "sparseVectorData = rdd2.map(lambda a :(a[0], sv_format(a[1])))\n",
    "myfile = sparseVectorData.collect()\n",
    "myfile1 = myfile[:int(len(myfile)/200)]\n",
    "my_df = pd.DataFrame(myfile1)\n",
    "my_df.to_csv('old.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this block of code generates random .5% of users with newer ratings\n",
    "\"\"\"\n",
    "distinct_val = new.select('item').distinct()\n",
    "with_index = distinct_val.rdd.zipWithIndex()\n",
    "distinct_val = with_index.map(lambda x: (Row(item = x[0].item, id =x[1])))\n",
    "distinct_val = distinct_val.toDF()\n",
    "new = new.orderBy(rand())\n",
    "joined_df = new.join(distinct_val, ['item'], \"left\")\n",
    "rdd1 = joined_df.rdd\n",
    "rdd2 = rdd1.map(lambda line: (line.user, (line.id, line.rating))).groupByKey().cache()\n",
    "\n",
    "def sv_format(x):\n",
    "    item = {}\n",
    "    for i in x:\n",
    "        item[i[0]] = int(float((i[1])))\n",
    "    return item\n",
    "\n",
    "sparseVectorData = rdd2.map(lambda a :(a[0], sv_format(a[1])))\n",
    "myfile = sparseVectorData.collect()\n",
    "myfile1 = myfile[:int(len(myfile)/200)]\n",
    "my_df = pd.DataFrame(myfile1)\n",
    "my_df.to_csv('new.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
